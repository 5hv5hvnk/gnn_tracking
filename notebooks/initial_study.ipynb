{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f8507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "\n",
    "# Externals\n",
    "import yaml\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import trackml.dataset\n",
    "from time import time\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU, Sigmoid\n",
    "from torch import optim\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# from torch_geometric.nn import GravNetConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f9eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, transform=None, pre_transform=None, graph_files=[]):\n",
    "        super(GraphDataset, self).__init__(None, transform, pre_transform)\n",
    "        self.graph_files = graph_files\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return self.graph_files\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return []\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.graph_files)\n",
    "\n",
    "    def get(self, idx):\n",
    "        with np.load(self.graph_files[idx]) as f:\n",
    "            # for k in f.iterkeys(): print(k)\n",
    "            x = torch.from_numpy(f[\"x\"])\n",
    "            edge_attr = torch.from_numpy(f[\"edge_attr\"])\n",
    "            edge_index = torch.from_numpy(f[\"edge_index\"])\n",
    "            y = torch.from_numpy(f[\"y\"])\n",
    "            particle_id = torch.from_numpy(f[\"particle_id\"])\n",
    "\n",
    "            # make graph undirected\n",
    "            row_0, col_0 = edge_index\n",
    "            row = torch.cat([row_0, col_0], dim=0)\n",
    "            col = torch.cat([col_0, row_0], dim=0)\n",
    "            edge_index = torch.stack([row, col], dim=0)\n",
    "            negate = torch.tensor([[-1], [-1], [-1], [1]])\n",
    "            edge_attr = torch.cat([edge_attr, negate * edge_attr], dim=1)\n",
    "            y = torch.cat([y, y])\n",
    "\n",
    "            data = Data(\n",
    "                x=x,\n",
    "                edge_index=edge_index,\n",
    "                edge_attr=torch.transpose(edge_attr, 0, 1),\n",
    "                y=y,\n",
    "                particle_id=particle_id,\n",
    "            )\n",
    "            data.num_nodes = len(x)\n",
    "\n",
    "        return (data, self.graph_files[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea2bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../graphs/train1_ptmin1\"\n",
    "graph_files = np.array(os.listdir(input_dir))\n",
    "graph_paths = np.array(\n",
    "    [os.path.join(input_dir, graph_file) for graph_file in graph_files]\n",
    ")\n",
    "# print(np.unique([f.split('00000')[-1].split('_')[0] for f in graph_paths ]))\n",
    "\n",
    "n_graphs = len(graph_files)\n",
    "IDs = np.arange(n_graphs)\n",
    "np.random.shuffle(IDs)\n",
    "partition = {\n",
    "    \"train\": graph_paths[IDs[:1500]],\n",
    "    \"test\": graph_paths[IDs[1500:1700]],\n",
    "    \"val\": graph_paths[IDs[1700:1750]],\n",
    "}\n",
    "params = {\"batch_size\": 1, \"shuffle\": True, \"num_workers\": 1}\n",
    "train_set = GraphDataset(graph_files=partition[\"train\"])\n",
    "train_loader = DataLoader(train_set, **params)\n",
    "test_set = GraphDataset(graph_files=partition[\"test\"])\n",
    "test_loader = DataLoader(test_set, **params)\n",
    "val_set = GraphDataset(graph_files=partition[\"val\"])\n",
    "val_loader = DataLoader(val_set, **params)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "for batch_idx, (data, f) in enumerate(train_loader):\n",
    "    data = data.to(device)\n",
    "    print(f, \"\\n\", data)\n",
    "    if batch_idx > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f93a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, C):\n",
    "        return self.layers(C)\n",
    "\n",
    "\n",
    "class RelationalModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(RelationalModel, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, m):\n",
    "        return self.layers(m)\n",
    "\n",
    "\n",
    "class ObjectModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(ObjectModel, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, C):\n",
    "        return self.layers(C)\n",
    "\n",
    "\n",
    "class InteractionNetwork(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_indim,\n",
    "        edge_indim,\n",
    "        node_outdim=3,\n",
    "        edge_outdim=4,\n",
    "        hidden_size=40,\n",
    "        aggr=\"add\",\n",
    "    ):\n",
    "        super(InteractionNetwork, self).__init__(aggr=aggr, flow=\"source_to_target\")\n",
    "        self.R1 = RelationalModel(2 * node_indim + edge_indim, edge_outdim, hidden_size)\n",
    "        self.O = ObjectModel(node_indim + edge_outdim, node_outdim, hidden_size)\n",
    "        self.E_tilde: Tensor = Tensor()\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "        # propagate_type: (x: Tensor, edge_attr: Tensor)\n",
    "        x_tilde = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=None)\n",
    "        return x_tilde, self.E_tilde\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        # x_i --> incoming, x_j --> outgoing\n",
    "        m1 = torch.cat([x_i, x_j, edge_attr], dim=1)\n",
    "        self.E_tilde = self.R1(m1)\n",
    "        return self.E_tilde\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        c = torch.cat([x, aggr_out], dim=1)\n",
    "        return self.O(c)\n",
    "\n",
    "\n",
    "# class GNN(nn.Module):\n",
    "#    def __init__(self):\n",
    "#        super(GNN, self).__init__()\n",
    "#        self.in1 = InteractionNetwork(3, 4, node_outdim=3,\n",
    "#                                      edge_outdim=4, hidden_size=80)\n",
    "#        self.in2 = InteractionNetwork(3, 4, node_outdim=3,\n",
    "#                                      edge_outdim=4, hidden_size=80,\n",
    "#                                      aggr='max')\n",
    "#        #self.in3 = InteractionNetwork(3, 4, node_outdim=3,\n",
    "#        #                              edge_outdim=4, hidden_size=100,\n",
    "#        #                              aggr='max')\n",
    "#        self.in_c = InteractionNetwork(3, 5, node_outdim=2,\n",
    "#                                       edge_outdim=4, hidden_size=80)\n",
    "#        self.W = MLP(12, 1, 80)\n",
    "#        self.B = MLP(2, 1, 40)\n",
    "#\n",
    "#    def forward(self, x: Tensor, edge_index: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "#        x1, edge_attr_1 = self.in1(x, edge_index, edge_attr)\n",
    "#        x2, edge_attr_2 = self.in2(x1, edge_index, edge_attr_1)\n",
    "#        #x3, edge_attr_3 = self.in3(x2, edge_index, edge_attr_2)\n",
    "#        all_edge_attr = torch.cat([edge_attr, edge_attr_1,\n",
    "#                                   edge_attr_2], dim=1) #edge_attr_3], dim=1)\n",
    "#\n",
    "#        edge_weights = torch.sigmoid(self.W(all_edge_attr))\n",
    "#        edge_attr_weights = torch.cat([edge_weights, edge_attr], dim=1)\n",
    "#        xc, edge_attr_c = self.in_c(x, edge_index, edge_attr_weights)\n",
    "#        beta = torch.sigmoid(self.B(xc))\n",
    "#        return edge_weights, xc, beta\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GNN, self).__init__()\n",
    "        self.in1 = InteractionNetwork(\n",
    "            3, 4, node_outdim=3, edge_outdim=4, hidden_size=50\n",
    "        )\n",
    "        self.in2 = InteractionNetwork(\n",
    "            3, 4, node_outdim=3, edge_outdim=4, hidden_size=50\n",
    "        )\n",
    "        self.in3 = InteractionNetwork(\n",
    "            3, 13, node_outdim=3, edge_outdim=8, hidden_size=50\n",
    "        )\n",
    "        self.in4 = InteractionNetwork(\n",
    "            3, 8, node_outdim=3, edge_outdim=8, hidden_size=50\n",
    "        )\n",
    "        self.in5 = InteractionNetwork(\n",
    "            3, 8, node_outdim=3, edge_outdim=8, hidden_size=50\n",
    "        )\n",
    "        self.W = MLP(12, 1, 80)\n",
    "        self.B = MLP(18, 1, 80)\n",
    "        self.X = MLP(18, 2, 80)\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "        x1, edge_attr_1 = self.in1(x, edge_index, edge_attr)\n",
    "        x2, edge_attr_2 = self.in2(x1, edge_index, edge_attr_1)\n",
    "\n",
    "        initial_edge_attr = torch.cat([edge_attr, edge_attr_1, edge_attr_2], dim=1)\n",
    "        edge_weights = torch.sigmoid(self.W(initial_edge_attr))\n",
    "        edge_attr_weights = torch.cat([edge_weights, initial_edge_attr], dim=1)\n",
    "\n",
    "        x3, edge_attr_3 = self.in3(x2, edge_index, edge_attr_weights)\n",
    "        x4, edge_attr_4 = self.in4(x3, edge_index, edge_attr_3)\n",
    "        x5, edge_attr_5 = self.in5(x4, edge_index, edge_attr_4)\n",
    "        all_x = torch.cat([x, x1, x2, x3, x4, x5], dim=1)\n",
    "        beta = torch.sigmoid(self.B(all_x))\n",
    "        Xc = self.X(all_x)\n",
    "        return edge_weights, Xc, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d7669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def V_attractive(x, x_alpha, q_alpha, device=\"cpu\"):\n",
    "    norm_sq = torch.norm(x - x_alpha, dim=1) ** 2\n",
    "    return norm_sq * q_alpha\n",
    "\n",
    "\n",
    "def V_repulsive(x, x_alpha, q_alpha, device=\"cpu\"):\n",
    "    diffs = 1 - torch.norm(x - x_alpha, dim=1)\n",
    "    hinges = torch.maximum(torch.zeros(len(x)).to(device), diffs)\n",
    "    return hinges * q_alpha\n",
    "\n",
    "\n",
    "def condensation_loss(beta, x, particle_id, device=\"cpu\", q_min=1):\n",
    "    loss = 0\n",
    "    q = torch.arctanh(beta) ** 2 + q_min\n",
    "    for pid in torch.unique(particle_id):\n",
    "        p = pid.item()\n",
    "        if p == 0:\n",
    "            continue\n",
    "        M = (particle_id == p).squeeze(-1)\n",
    "        q_pid = q[M]\n",
    "        x_pid = x[M]\n",
    "        M = M.long()\n",
    "        alpha = torch.argmax(q_pid)\n",
    "        q_alpha = q_pid[alpha]\n",
    "        x_alpha = x_pid[alpha]\n",
    "        va = V_attractive(x, x_alpha, q_alpha, device=device)\n",
    "        vr = V_repulsive(x, x_alpha, q_alpha, device=device)\n",
    "        loss += torch.mean(q * (M * va + 10 * (1 - M) * vr))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def background_loss(beta, x, particle_id, device=\"cpu\", q_min=1, sb=10):\n",
    "    loss = 0\n",
    "    unique_pids = torch.unique(particle_id)\n",
    "    beta_alphas = torch.zeros(len(unique_pids)).to(device)\n",
    "    for i, pid in enumerate(unique_pids):\n",
    "        p = pid.item()\n",
    "        if p == 0:\n",
    "            continue\n",
    "        M = (particle_id == p).squeeze(-1)\n",
    "        beta_pid = beta[M]\n",
    "        alpha = torch.argmax(beta_pid)\n",
    "        beta_alpha = beta_pid[alpha]\n",
    "        beta_alphas[i] = beta_alpha\n",
    "\n",
    "    n = (particle_id == 0).long()\n",
    "    nb = torch.sum(n)\n",
    "    if nb == 0:\n",
    "        return torch.tensor(0)\n",
    "    return torch.mean(1 - beta_alphas) + sb * torch.sum(n * beta) / nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed9bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, val_loader):\n",
    "    model.eval()\n",
    "    opt_thlds, accs = [], []\n",
    "    for batch_idx, (data, f) in enumerate(val_loader):\n",
    "        data = data.to(device)\n",
    "        output, xc, beta = model(data.x, data.edge_index, data.edge_attr)\n",
    "        y, output = data.y, output.squeeze()\n",
    "        loss = F.binary_cross_entropy(output, y, reduction=\"mean\").item()\n",
    "\n",
    "        # define optimal threshold (thld) where TPR = TNR\n",
    "        diff, opt_thld, opt_acc = 100, 0, 0\n",
    "        best_tpr, best_tnr = 0, 0\n",
    "        for thld in np.arange(0.01, 0.5, 0.001):\n",
    "            TP = torch.sum((y == 1) & (output > thld)).item()\n",
    "            TN = torch.sum((y == 0) & (output < thld)).item()\n",
    "            FP = torch.sum((y == 0) & (output >= thld)).item()\n",
    "            FN = torch.sum((y == 1) & (output <= thld)).item()\n",
    "            # print(thld, 'tp', TP, 'tn', TN, 'fp', FP, 'fn', FN)\n",
    "            acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "            if (TP + FN == 0) or (TN + FP == 0):\n",
    "                opt_thld, opt_acc = thld, 1\n",
    "                break\n",
    "            TPR, TNR = TP / (TP + FN), TN / (TN + FP)\n",
    "            delta = abs(TPR - TNR)\n",
    "            if delta < diff:\n",
    "                diff, opt_thld, opt_acc = delta, thld, acc\n",
    "\n",
    "        opt_thlds.append(opt_thld)\n",
    "        accs.append(opt_acc)\n",
    "\n",
    "    print(f\"...optimal edge weight threshold: {np.mean(opt_thlds):.5f}\")\n",
    "    return np.nanmean(opt_thlds)\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, thld=0.5):\n",
    "    model.eval()\n",
    "    losses, losses_w, losses_c, losses_b, accs = [], [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, f) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            output, xc, beta = model(data.x, data.edge_index, data.edge_attr)\n",
    "\n",
    "            # edge classification accuracy\n",
    "            TP = torch.sum((data.y == 1).squeeze() & (output > thld).squeeze()).item()\n",
    "            TN = torch.sum((data.y == 0).squeeze() & (output < thld).squeeze()).item()\n",
    "            FP = torch.sum((data.y == 0).squeeze() & (output > thld).squeeze()).item()\n",
    "            FN = torch.sum((data.y == 1).squeeze() & (output < thld).squeeze()).item()\n",
    "            if (TP + TN + FP + FN) != 0:\n",
    "                acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "                accs.append(acc)\n",
    "\n",
    "            # edge weight loss\n",
    "            loss = F.binary_cross_entropy(\n",
    "                output.squeeze(1), data.y, reduction=\"mean\"\n",
    "            ).item()\n",
    "            losses_w.append(loss)\n",
    "\n",
    "            # condensation loss\n",
    "            particle_id = data.particle_id\n",
    "            loss_c = (\n",
    "                condensation_loss(beta, xc, particle_id, device=device, q_min=0.05) * 10\n",
    "            )\n",
    "            losses_c.append(loss_c.item())\n",
    "            loss_b = (\n",
    "                background_loss(beta, xc, particle_id, device=\"cpu\", q_min=0.05, sb=1)\n",
    "                / 500\n",
    "            )\n",
    "            losses_b.append(loss_b.item())\n",
    "\n",
    "            # total loss\n",
    "            loss += loss_c\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    print(f\"...test total loss: {np.mean(losses):.5f}\")\n",
    "    print(f\"...test edge weight loss: {np.mean(losses_w):.5f}\")\n",
    "    print(f\"...test edge classification accuracy: {np.mean(accs):.5f}\")\n",
    "    print(f\"...test condensation loss: {np.mean(losses_c):.5f}\")\n",
    "    print(f\"...test background loss: {np.mean(losses_b):.5f}\")\n",
    "    return np.nanmean(losses), np.nanmean(accs)\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    epoch_t0 = time()\n",
    "    losses, losses_w, losses_c, losses_b = [], [], [], []\n",
    "    for batch_idx, (data, f) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, xc, beta = model(data.x, data.edge_index, data.edge_attr)\n",
    "        y, output = data.y, output.squeeze(1)\n",
    "        particle_id = data.particle_id\n",
    "        loss_w = F.binary_cross_entropy(output, y, reduction=\"mean\")\n",
    "        loss = loss_w\n",
    "        loss_c = (\n",
    "            condensation_loss(beta, xc, particle_id, device=device, q_min=0.05) / 10\n",
    "        )  # (10**2)\n",
    "        loss_b = (\n",
    "            background_loss(beta, xc, particle_id, device=\"cpu\", q_min=0.05, sb=1) / 500\n",
    "        )\n",
    "        loss += loss_c\n",
    "        loss += loss_b\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args[\"log_interval\"] == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx,\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "        losses.append(loss.item())\n",
    "        losses_w.append(loss_w.item())\n",
    "        losses_c.append(loss_c.item())\n",
    "        losses_b.append(loss_b.item())\n",
    "    print(f\"...epoch time: {(time()-epoch_t0):.4f}s\")\n",
    "    print(f\"...epoch {epoch}: train loss={np.nanmean(losses):.6f}\")\n",
    "    print(f\"...epoch {epoch}: bce loss={np.nanmean(loss_w):.6f}\")\n",
    "    print(f\"...epoch {epoch}: condensation loss={np.nanmean(loss_c):.6f}\")\n",
    "    print(f\"...epoch {epoch}: background loss={np.nanmean(loss_b):.6f}\")\n",
    "    return np.nanmean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1526cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"lr\": 0.0005, \"gamma\": 0.95, \"step_size\": 5, \"log_interval\": 10, \"epochs\": 250}\n",
    "# model = InteractionNetwork(3,4,hidden_size=60).to(device)\n",
    "model = GNN().to(device)\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"total trainable params:\", total_trainable_params)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "scheduler = StepLR(optimizer, step_size=args[\"step_size\"], gamma=args[\"gamma\"])\n",
    "\n",
    "output = {\"train_loss\": [], \"test_loss\": [], \"test_acc\": []}\n",
    "for epoch in range(1, args[\"epochs\"] + 1):\n",
    "    print(\"---- Epoch {} ----\".format(epoch))\n",
    "    train_loss = train(args, model, device, train_loader, optimizer, epoch)\n",
    "    thld = validate(model, device, val_loader)\n",
    "    print(\"...optimal threshold\", thld)\n",
    "    test_loss, test_acc = test(model, device, test_loader, thld=thld)\n",
    "    scheduler.step()\n",
    "    torch.save(\n",
    "        model.state_dict(), \"../trained_models/ptmin_0GeV_epoch{}_new.pt\".format(epoch)\n",
    "    )\n",
    "\n",
    "    output[\"train_loss\"].append(train_loss)\n",
    "    output[\"test_loss\"].append(test_loss)\n",
    "    output[\"test_acc\"].append(test_acc)\n",
    "    np.save(\"../trained_models/train_stats/ptmin_0GeV_new\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.pyplot import cm\n",
    "from sklearn.cluster import DBSCAN\n",
    "from random import shuffle\n",
    "from collections import Counter\n",
    "import mplhep as hep\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n",
    "\n",
    "colors = cm.Paired.colors\n",
    "losses_w = [\n",
    "    0.01471,\n",
    "    0.01401,\n",
    "    0.01153,\n",
    "    0.01085,\n",
    "    0.01440,\n",
    "    0.01013,\n",
    "    0.02281,\n",
    "    0.01054,\n",
    "    0.00916,\n",
    "    0.00856,\n",
    "    0.00828,\n",
    "    0.00796,\n",
    "    0.00790,\n",
    "    0.00792,\n",
    "    0.00786,\n",
    "    0.01541,\n",
    "    0.00766,\n",
    "    0.00850,\n",
    "    0.00751,\n",
    "    0.00809,\n",
    "    0.00736,\n",
    "    0.00727,\n",
    "    0.00723,\n",
    "    0.00751,\n",
    "    0.00711,\n",
    "    0.00697,\n",
    "    0.00743,\n",
    "    0.00705,\n",
    "]\n",
    "losses_c = [\n",
    "    0.05393,\n",
    "    0.0227,\n",
    "    0.02121,\n",
    "    0.01774,\n",
    "    0.01698,\n",
    "    0.01486,\n",
    "    0.01572,\n",
    "    0.01370,\n",
    "    0.01370,\n",
    "    0.01352,\n",
    "    0.01216,\n",
    "    0.01218,\n",
    "    0.01167,\n",
    "    0.01220,\n",
    "    0.01190,\n",
    "    0.01380,\n",
    "    0.01168,\n",
    "    0.01173,\n",
    "    0.01186,\n",
    "    0.01137,\n",
    "    0.01135,\n",
    "    0.01244,\n",
    "    0.01108,\n",
    "    0.01054,\n",
    "    0.01145,\n",
    "    0.01015,\n",
    "    0.01044,\n",
    "    0.01066,\n",
    "]\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(14, 14), dpi=200, sharex=True)\n",
    "train_out = np.load(\n",
    "    \"../trained_models/ORF543/train_stats/ptmin_1GeV_new.npy\", allow_pickle=True\n",
    ").item()\n",
    "n_epoch = len(train_out[\"test_loss\"])\n",
    "epochs = np.arange(1, n_epoch + 1, 1)\n",
    "axs[0].plot(\n",
    "    train_out[\"train_loss\"],\n",
    "    markersize=15,\n",
    "    linewidth=2,\n",
    "    linestyle=\"-\",\n",
    "    label=\"train loss\",\n",
    ")\n",
    "axs[0].plot(\n",
    "    train_out[\"test_loss\"],\n",
    "    markersize=15,\n",
    "    linewidth=2,\n",
    "    linestyle=\"-\",\n",
    "    label=\"test loss ($\\mathcal{L}$)\",\n",
    ")\n",
    "axs[0].plot(\n",
    "    losses_w,\n",
    "    markersize=15,\n",
    "    linewidth=2,\n",
    "    linestyle=\"-\",\n",
    "    label=\"test BCE loss ($\\mathcal{L}_w$)\",\n",
    ")\n",
    "axs[0].plot(\n",
    "    losses_c,\n",
    "    markersize=15,\n",
    "    linewidth=2,\n",
    "    linestyle=\"-\",\n",
    "    label=\"test condensation loss ($\\mathcal{L}_c$)\",\n",
    ")\n",
    "axs[1].plot(\n",
    "    train_out[\"test_acc\"],\n",
    "    markersize=15,\n",
    "    linewidth=2,\n",
    "    linestyle=\"-\",\n",
    "    label=\"test accuracy\",\n",
    ")\n",
    "\n",
    "# axs[0].legend(loc='best')\n",
    "# axs[0].set_ylim([0.001,0.007])\n",
    "\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "# axs[0].set_xlabel('Epoch')\n",
    "axs[0].ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n",
    "axs[0].set_ylim([0, 0.05])\n",
    "\n",
    "# axs[1].set_ylim([0.9965,0.999])\n",
    "axs[1].set_ylabel(\"Edge Classification Accuracy\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "# axs[1].set_yticks([0.9965,0.997,0.9975,0.998,0.9985,0.999])\n",
    "\n",
    "# plt.savefig('paper_plots/training_convergence.pdf', format='PDF', bbox_inches='tight')\n",
    "axs[0].legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd0e8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the results on a single event (1200)\n",
    "device = \"cpu\"\n",
    "model_dir = \"../trained_models/ORF543\"\n",
    "model = \"ptmin_1GeV_epoch46_new.pt\"\n",
    "gnn = GNN().to(device)\n",
    "gnn.load_state_dict(\n",
    "    torch.load(os.path.join(model_dir, model), map_location=torch.device(device))\n",
    ")\n",
    "\n",
    "input_dir = \"../graphs/train1_ptmin1\"\n",
    "evt_of_interest = \"1200\"\n",
    "graph_files = np.array(os.listdir(input_dir))\n",
    "graph_paths = np.array(\n",
    "    [\n",
    "        os.path.join(input_dir, graph_file)\n",
    "        for graph_file in graph_files\n",
    "        if evt_of_interest in graph_file\n",
    "    ]\n",
    ")\n",
    "print(f\"examining {len(graph_paths)} files:\\n\", graph_paths)\n",
    "\n",
    "params = {\"batch_size\": 1, \"shuffle\": True, \"num_workers\": 1}\n",
    "test_set = GraphDataset(graph_files=graph_paths)\n",
    "test_loader = DataLoader(test_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a3071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn.eval()\n",
    "thld = 0.153  # from validation sample\n",
    "output = {\"test_loss\": [], \"test_acc\": []}\n",
    "# eps_per_sector = {'2': 0.3564241822816778, '22': 0.45843991876458323, '30': 0.4169384299062302, '29': 0.38705057106242907, '26': 0.3455041866230518, '10': 0.36766370445586516, '1': 0.3036541924266648, '48': 0.18149576536563505, '25': 0.19704120326178534, '41': 0.26664902466066226, '56': 0.2157166192270978, '53': 0.4097118708407425, '14': 0.5776026396967548, '52': 0.4897099302911026, '17': 0.32234367029956457, '37': 0.45964267052796437, '47': 0.20748764032469144, '40': 0.11834033780629383, '50': 0.3648087553192596, '43': 0.6117603874427162, '57': 0.2814069607914131, '39': 0.19480088644724136, '4': 0.4797073653156661, '20': 0.4320027929704173, '36': 0.6628498681482498, '62': 0.30954201040247753, '28': 0.5171349701504945, '42': 0.418948477000258, '15': 0.2508787160267514, '16': 0.21156652945703436, '21': 0.41145608668912287, '35': 0.5350950945118965, '12': 0.4732929783854891, '33': 0.2509914058302759, '34': 0.3345035199042572, '54': 0.3544828443971243, '6': 0.3605703464644229, '45': 0.4970332493010159, '23': 0.15307456520675453, '9': 0.2928337271520215, '63': 0.21375156675561108, '27': 0.4600810743406718, '13': 0.33493758583269617, '49': 0.23781110190043425, '0': 0.1672453080584718, '44': 0.6048629114697928, '58': 0.37508685961178423, '61': 0.376480475496482, '55': 0.24918232797162998, '60': 0.45978119605346224, '31': 0.4860280909701997, '18': 0.3524482894718338, '19': 0.36911811362641156, '38': 0.384640685382996, '24': 0.17406356539277051, '5': 0.3427366006854611, '32': 0.18318814309046946, '3': 0.4765896040481174, '8': 0.13300241176133498, '7': 0.27902516215008294, '51': 0.4240012533714197, '11': 0.4463760096499357, '46': 0.3890997229178218, '59': 0.7288422692248555}\n",
    "# eps_per_sector = {'2': 0.8063548272690922, '22': 0.299405179960324, '30': 0.28859853655239975, '29': 0.33339633574780086, '26': 0.5236165326325144, '10': 0.5216460111562207, '1': 0.2878294081123941, '48': 0.22263953508649145, '25': 0.34931555338021225, '41': 0.33230337108552677, '56': 0.20547030457458215, '53': 0.3559793988629558, '14': 0.24844131669585412, '52': 0.5608325417346683, '17': 0.3456908316257858, '37': 0.3922355410981205, '47': 0.2039827087386243, '40': 0.19807322950134404, '50': 1.1135923658563385, '43': 0.5331037394268494, '57': 0.2710876333023264, '39': 0.19920797777676114, '4': 0.44904851441225124, '20': 0.602187737520835, '36': 0.5466592138502525, '62': 0.24127534088551628, '28': 0.5347594761474337, '42': 0.41772241226028095, '15': 0.24230064272641902, '16': 0.1943095736137368, '21': 1.0508085294451583, '35': 0.378743656658796, '12': 0.5591883019759802, '33': 0.33630823925312137, '34': 0.43069772694509106, '54': 0.2785143389761452, '6': 0.306822059393292, '45': 0.46596615711030376, '23': 0.19588896863672595, '9': 0.3572548379207321, '63': 0.17333863999794852, '27': 0.6171556193101095, '13': 0.41517929321316493, '49': 0.42404231447481017, '0': 0.2740409028474836, '44': 0.4495038154213366, '58': 0.676606553714549, '61': 0.6486801322347483, '55': 0.2262980339273397, '60': 0.5528389471875124, '31': 0.22853232441126992, '18': 0.44848417813676533, '19': 0.4531880461432393, '38': 0.2857591954424449, '24': 0.2684531593154434, '5': 0.4045195515657686, '32': 0.21747424924573708, '3': 0.5348996145154985, '8': 0.2211579281046795, '7': 0.24975328523382798, '51': 0.5185944888548765, '11': 0.5056706399944967, '46': 0.25581046164540183, '59': 1.1737162383743445}\n",
    "eps_per_sector = {\n",
    "    \"2\": 0.5495896408725386,\n",
    "    \"22\": 0.2066768835039086,\n",
    "    \"30\": 0.32207736849433066,\n",
    "    \"29\": 0.2153735994036366,\n",
    "    \"26\": 0.33333082879262993,\n",
    "    \"10\": 0.4483006357560373,\n",
    "    \"1\": 0.15084099122235584,\n",
    "    \"48\": 0.09133902135173017,\n",
    "    \"25\": 0.1851028928201932,\n",
    "    \"41\": 0.21531603307034636,\n",
    "    \"56\": 0.09517457366962045,\n",
    "    \"53\": 0.26678582887569857,\n",
    "    \"14\": 0.22142935883358877,\n",
    "    \"52\": 0.3418372561998032,\n",
    "    \"17\": 0.2212906430447033,\n",
    "    \"37\": 0.19312759435470764,\n",
    "    \"47\": 0.07242282075561883,\n",
    "    \"40\": 0.043647137743399386,\n",
    "    \"50\": 0.7699143485549735,\n",
    "    \"43\": 0.3503989728702347,\n",
    "    \"57\": 0.16920986038063074,\n",
    "    \"39\": 0.0439630494567799,\n",
    "    \"4\": 0.2022154563216964,\n",
    "    \"20\": 0.3449003159515353,\n",
    "    \"36\": 0.2526325497354306,\n",
    "    \"62\": 0.13945573208896572,\n",
    "    \"28\": 0.33674114954372225,\n",
    "    \"42\": 0.2552393778183979,\n",
    "    \"15\": 0.10452628934529434,\n",
    "    \"16\": 0.10424390260245978,\n",
    "    \"21\": 0.7380104344215415,\n",
    "    \"35\": 0.1978508778392623,\n",
    "    \"12\": 0.2676435293824886,\n",
    "    \"33\": 0.2696216773866944,\n",
    "    \"34\": 0.22186430182786004,\n",
    "    \"54\": 0.1762268184405068,\n",
    "    \"6\": 0.24255059505853188,\n",
    "    \"45\": 0.2674585713555141,\n",
    "    \"23\": 0.10362311667582357,\n",
    "    \"9\": 0.235383881318328,\n",
    "    \"63\": 0.08119582817237184,\n",
    "    \"27\": 0.3838984846446019,\n",
    "    \"13\": 0.3159058507535464,\n",
    "    \"49\": 0.18657413342638907,\n",
    "    \"0\": 0.11556407496284879,\n",
    "    \"44\": 0.2883726398338603,\n",
    "    \"58\": 0.42306760019056955,\n",
    "    \"61\": 0.42685130034012675,\n",
    "    \"55\": 0.11039074677407099,\n",
    "    \"60\": 0.3684585347152781,\n",
    "    \"31\": 0.11313115839546933,\n",
    "    \"18\": 0.22274020570291234,\n",
    "    \"19\": 0.2577446903341364,\n",
    "    \"38\": 0.23418950808364947,\n",
    "    \"24\": 0.10320896502403629,\n",
    "    \"5\": 0.23951099655502492,\n",
    "    \"32\": 0.10256076156375457,\n",
    "    \"3\": 0.3988297609696611,\n",
    "    \"8\": 0.13298719725169572,\n",
    "    \"7\": 0.10731956348135153,\n",
    "    \"51\": 0.34252400841044667,\n",
    "    \"11\": 0.3431829658915274,\n",
    "    \"46\": 0.19143257049887802,\n",
    "    \"59\": 0.7589238164237745,\n",
    "}\n",
    "summary_stats = {\n",
    "    \"n_pids\": 0,\n",
    "    \"perfect_matches\": 0,\n",
    "    \"double_majority\": 0,\n",
    "    \"lhc_loose_matches\": 0,\n",
    "}\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, f) in enumerate(test_loader):\n",
    "        print(f)\n",
    "        sector = f[0].split(\"_s\")[-1].split(\".npz\")[0]\n",
    "        print(\"analyzing sector\", sector)\n",
    "        data = data.to(device)\n",
    "        out, xc, beta = gnn(data.x, data.edge_index, data.edge_attr)\n",
    "        y, out = data.y, out.squeeze(1)\n",
    "        particle_id = data.particle_id\n",
    "\n",
    "        #######################################\n",
    "        # output event file for DBScan studies\n",
    "        d = {\"x1\": xc[:, 0], \"x2\": xc[:, 1], \"particle_id\": particle_id}\n",
    "        df = pd.DataFrame(data=d)\n",
    "        # df.to_csv(f'../cluster_predictions/event{evt_of_interest}_s{sector}_ptmin1GeV.csv', index=False)\n",
    "        #######################################\n",
    "\n",
    "        loss = F.binary_cross_entropy(out, y, reduction=\"mean\")\n",
    "        loss_c = (\n",
    "            condensation_loss(beta, xc, particle_id, device=device, q_min=0.05) * 10\n",
    "        )  # (10**2)\n",
    "        print(\"condensation loss\", loss_c.item())\n",
    "        loss += loss_c\n",
    "\n",
    "        TP = torch.sum((y == 1) & (out > thld)).item()\n",
    "        TN = torch.sum((y == 0) & (out < thld)).item()\n",
    "        FP = torch.sum((y == 0) & (out > thld)).item()\n",
    "        FN = torch.sum((y == 1) & (out < thld)).item()\n",
    "        acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "        print(\"edge weight loss\", loss)\n",
    "        print(\"condensation loss\", loss_c)\n",
    "        output[\"test_acc\"].append(acc)\n",
    "        output[\"test_loss\"].append(loss)\n",
    "\n",
    "        n_pids = len(torch.unique(particle_id))\n",
    "        nhits_per_pid = {}\n",
    "        single_hit_particles = 0\n",
    "        for pid in torch.unique(particle_id):\n",
    "            nhits = torch.sum(particle_id == pid).item()\n",
    "            if nhits == 1:\n",
    "                single_hit_particles += 1\n",
    "            nhits_per_pid[pid.item()] = nhits\n",
    "        print(\"number of particles\", n_pids)\n",
    "        print(\"number of tracks:\", n_pids - single_hit_particles)\n",
    "\n",
    "        db = DBSCAN(eps=eps_per_sector[sector], min_samples=2).fit(xc)\n",
    "        labels = db.labels_\n",
    "        n_clusters = len(np.unique(labels))\n",
    "        print(\"number of labels:\", n_clusters)\n",
    "        cmap = list(cm.tab20c(np.linspace(0, 1, n_clusters)))\n",
    "\n",
    "        perfect_matches = 0\n",
    "        double_majority = 0\n",
    "        lhc_loose_matches = 0\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(16, 8), dpi=200)\n",
    "        for label in np.unique(labels):\n",
    "            xc_cluster = xc[labels == label]\n",
    "            x_cluster = data.x[labels == label].numpy()\n",
    "            axs[0].scatter(\n",
    "                xc_cluster[:, 0], xc_cluster[:, 1], color=cmap[label], marker=\".\"\n",
    "            )\n",
    "\n",
    "            idxs = np.argsort(x_cluster[:, 0])\n",
    "            r = x_cluster[:, 0][idxs]\n",
    "            z = x_cluster[:, 2][idxs]\n",
    "            axs[1].scatter(z, r, color=cmap[label], marker=\".\")\n",
    "            for i in range(len(r) - 1):\n",
    "                axs[1].plot((z[i], z[i + 1]), (r[i], r[i + 1]), color=cmap[label])\n",
    "\n",
    "            # analyze reconstruction\n",
    "            pid_cluster = particle_id[labels == label].tolist()\n",
    "            N = len(pid_cluster)\n",
    "            pid_counts = Counter(pid_cluster)\n",
    "\n",
    "            # handle the noise case\n",
    "            if label == -1:\n",
    "                continue\n",
    "\n",
    "            # otherwise, clusters should be tracks\n",
    "            most_common = pid_counts.most_common(1)[0]  # e.g. (810, 8)\n",
    "            ntruth = nhits_per_pid[most_common[0]]\n",
    "            if (N == most_common[1]) and (ntruth == most_common[1]):\n",
    "                perfect_matches += 1\n",
    "            if (most_common[1] / N > 0.5) and (most_common[1] / ntruth > 0.5):\n",
    "                double_majority += 1\n",
    "            if most_common[1] / ntruth > 0.75:\n",
    "                lhc_loose_matches += 1\n",
    "\n",
    "            # systematic way to tune DBSCAN eps parameter\n",
    "            # *** note that this must be run on a separate graph!\n",
    "            # dists_per_sector = []\n",
    "            # dists_std_per_sector = []\n",
    "            # for pid in particle_id:\n",
    "            #    xc_cluster = xc[particle_id==pid]\n",
    "            #    dists = []\n",
    "            #    xc0 = xc_cluster[:,0].numpy()\n",
    "            #    xc1 = xc_cluster[:,1].numpy()\n",
    "            #    for i in range(len(xc0)):\n",
    "            #        for j in range(len(xc0)):\n",
    "            #            if j>=i: continue\n",
    "            #            dists.append(np.sqrt((xc0[i]-xc0[j])**2 +\n",
    "            #                                 (xc1[i]-xc1[j])**2))\n",
    "\n",
    "            #    dists_per_sector.append(np.nanmean(dists))\n",
    "            #    dists_std_per_sector.append(np.nanstd(dists))\n",
    "\n",
    "        # print('dists_per_sector', np.nanmean(dists_per_sector))\n",
    "        # print('dists_std_per_sector', np.nanstd(dists_std_per_sector))\n",
    "        # eps_per_sector[sector] = np.nanmean(dists_per_sector) + np.nanmean(dists_std_per_sector)\n",
    "\n",
    "        denominator = n_pids - single_hit_particles\n",
    "        print(\"perfect match fraction:\", perfect_matches / denominator)\n",
    "        print(\"double majority fraction:\", double_majority / denominator)\n",
    "        print(\"lhc loose fraction:\", lhc_loose_matches / denominator)\n",
    "        summary_stats[\"n_pids\"] += n_pids - single_hit_particles\n",
    "        summary_stats[\"perfect_matches\"] += perfect_matches\n",
    "        summary_stats[\"double_majority\"] += double_majority\n",
    "        summary_stats[\"lhc_loose_matches\"] += lhc_loose_matches\n",
    "        axs[0].set_xlabel(\"$x_1$\")\n",
    "        axs[0].set_ylabel(\"$x_2$\")\n",
    "        axs[1].set_xlabel(\"$z$\")\n",
    "        axs[1].set_ylabel(\"$r$\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        # plt.savefig(f'../cluster_predictions/event{evt_of_interest}_s{sector}_ptmin1GeV.png', dpi=200)\n",
    "    # print(eps_per_sector)\n",
    "\n",
    "# entire event summary\n",
    "print(\"summary of full event\")\n",
    "n_pids = summary_stats[\"n_pids\"]\n",
    "print(\"perfect match fraction:\", summary_stats[\"perfect_matches\"] / n_pids)\n",
    "print(\"double majority fraction:\", summary_stats[\"double_majority\"] / n_pids)\n",
    "print(\"lhc loose fraction:\", summary_stats[\"lhc_loose_matches\"] / n_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef6eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now calculate the efficiencies per sector across an unseen sample of graphs\n",
    "gnn.eval()\n",
    "thld = 0.153\n",
    "output = {\"test_loss\": [], \"test_acc\": []}\n",
    "eps_per_sector = {\n",
    "    \"2\": 0.8063548272690922,\n",
    "    \"22\": 0.299405179960324,\n",
    "    \"30\": 0.28859853655239975,\n",
    "    \"29\": 0.33339633574780086,\n",
    "    \"26\": 0.5236165326325144,\n",
    "    \"10\": 0.5216460111562207,\n",
    "    \"1\": 0.2878294081123941,\n",
    "    \"48\": 0.22263953508649145,\n",
    "    \"25\": 0.34931555338021225,\n",
    "    \"41\": 0.33230337108552677,\n",
    "    \"56\": 0.20547030457458215,\n",
    "    \"53\": 0.3559793988629558,\n",
    "    \"14\": 0.24844131669585412,\n",
    "    \"52\": 0.5608325417346683,\n",
    "    \"17\": 0.3456908316257858,\n",
    "    \"37\": 0.3922355410981205,\n",
    "    \"47\": 0.2039827087386243,\n",
    "    \"40\": 0.19807322950134404,\n",
    "    \"50\": 1.1135923658563385,\n",
    "    \"43\": 0.5331037394268494,\n",
    "    \"57\": 0.2710876333023264,\n",
    "    \"39\": 0.19920797777676114,\n",
    "    \"4\": 0.44904851441225124,\n",
    "    \"20\": 0.602187737520835,\n",
    "    \"36\": 0.5466592138502525,\n",
    "    \"62\": 0.24127534088551628,\n",
    "    \"28\": 0.5347594761474337,\n",
    "    \"42\": 0.41772241226028095,\n",
    "    \"15\": 0.24230064272641902,\n",
    "    \"16\": 0.1943095736137368,\n",
    "    \"21\": 1.0508085294451583,\n",
    "    \"35\": 0.378743656658796,\n",
    "    \"12\": 0.5591883019759802,\n",
    "    \"33\": 0.33630823925312137,\n",
    "    \"34\": 0.43069772694509106,\n",
    "    \"54\": 0.2785143389761452,\n",
    "    \"6\": 0.306822059393292,\n",
    "    \"45\": 0.46596615711030376,\n",
    "    \"23\": 0.19588896863672595,\n",
    "    \"9\": 0.3572548379207321,\n",
    "    \"63\": 0.17333863999794852,\n",
    "    \"27\": 0.6171556193101095,\n",
    "    \"13\": 0.41517929321316493,\n",
    "    \"49\": 0.42404231447481017,\n",
    "    \"0\": 0.2740409028474836,\n",
    "    \"44\": 0.4495038154213366,\n",
    "    \"58\": 0.676606553714549,\n",
    "    \"61\": 0.6486801322347483,\n",
    "    \"55\": 0.2262980339273397,\n",
    "    \"60\": 0.5528389471875124,\n",
    "    \"31\": 0.22853232441126992,\n",
    "    \"18\": 0.44848417813676533,\n",
    "    \"19\": 0.4531880461432393,\n",
    "    \"38\": 0.2857591954424449,\n",
    "    \"24\": 0.2684531593154434,\n",
    "    \"5\": 0.4045195515657686,\n",
    "    \"32\": 0.21747424924573708,\n",
    "    \"3\": 0.5348996145154985,\n",
    "    \"8\": 0.2211579281046795,\n",
    "    \"7\": 0.24975328523382798,\n",
    "    \"51\": 0.5185944888548765,\n",
    "    \"11\": 0.5056706399944967,\n",
    "    \"46\": 0.25581046164540183,\n",
    "    \"59\": 1.1737162383743445,\n",
    "}\n",
    "# eps_per_sector = {'2': 0.5495896408725386, '22': 0.2066768835039086, '30': 0.32207736849433066, '29': 0.2153735994036366, '26': 0.33333082879262993, '10': 0.4483006357560373, '1': 0.15084099122235584, '48': 0.09133902135173017, '25': 0.1851028928201932, '41': 0.21531603307034636, '56': 0.09517457366962045, '53': 0.26678582887569857, '14': 0.22142935883358877, '52': 0.3418372561998032, '17': 0.2212906430447033, '37': 0.19312759435470764, '47': 0.07242282075561883, '40': 0.043647137743399386, '50': 0.7699143485549735, '43': 0.3503989728702347, '57': 0.16920986038063074, '39': 0.0439630494567799, '4': 0.2022154563216964, '20': 0.3449003159515353, '36': 0.2526325497354306, '62': 0.13945573208896572, '28': 0.33674114954372225, '42': 0.2552393778183979, '15': 0.10452628934529434, '16': 0.10424390260245978, '21': 0.7380104344215415, '35': 0.1978508778392623, '12': 0.2676435293824886, '33': 0.2696216773866944, '34': 0.22186430182786004, '54': 0.1762268184405068, '6': 0.24255059505853188, '45': 0.2674585713555141, '23': 0.10362311667582357, '9': 0.235383881318328, '63': 0.08119582817237184, '27': 0.3838984846446019, '13': 0.3159058507535464, '49': 0.18657413342638907, '0': 0.11556407496284879, '44': 0.2883726398338603, '58': 0.42306760019056955, '61': 0.42685130034012675, '55': 0.11039074677407099, '60': 0.3684585347152781, '31': 0.11313115839546933, '18': 0.22274020570291234, '19': 0.2577446903341364, '38': 0.23418950808364947, '24': 0.10320896502403629, '5': 0.23951099655502492, '32': 0.10256076156375457, '3': 0.3988297609696611, '8': 0.13298719725169572, '7': 0.10731956348135153, '51': 0.34252400841044667, '11': 0.3431829658915274, '46': 0.19143257049887802, '59': 0.7589238164237745}\n",
    "\n",
    "summary_stats = {\n",
    "    \"n_pids\": 0,\n",
    "    \"perfect_matches\": 0,\n",
    "    \"double_majority\": 0,\n",
    "    \"lhc_loose_matches\": 0,\n",
    "}\n",
    "\n",
    "input_dir = \"../graphs/ptmin_1GeV_train2\"\n",
    "graph_files = np.array(os.listdir(input_dir))\n",
    "graph_paths = np.array(\n",
    "    [os.path.join(input_dir, graph_file) for graph_file in graph_files]\n",
    ")\n",
    "\n",
    "params = {\"batch_size\": 1, \"shuffle\": True, \"num_workers\": 1}\n",
    "test_set = GraphDataset(graph_files=graph_paths)\n",
    "test_loader = DataLoader(test_set, **params)\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, f) in enumerate(test_loader):\n",
    "        sector = f[0].split(\"_s\")[-1].split(\".npz\")[0]\n",
    "        # print('analyzing sector', sector)\n",
    "        data = data.to(device)\n",
    "        out, xc, beta = gnn(data.x, data.edge_index, data.edge_attr)\n",
    "        y, out = data.y, out.squeeze(1)\n",
    "        particle_id = data.particle_id\n",
    "        loss = F.binary_cross_entropy(out, y, reduction=\"mean\")\n",
    "        loss_c = (\n",
    "            condensation_loss(beta, xc, particle_id, device=device, q_min=0.05) * 10\n",
    "        )  # (10**2)\n",
    "        # print('condensation loss', loss_c.item())\n",
    "        loss += loss_c\n",
    "\n",
    "        TP = torch.sum((y == 1) & (out > thld)).item()\n",
    "        TN = torch.sum((y == 0) & (out < thld)).item()\n",
    "        FP = torch.sum((y == 0) & (out > thld)).item()\n",
    "        FN = torch.sum((y == 1) & (out < thld)).item()\n",
    "        acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "        output[\"test_acc\"].append(acc)\n",
    "        output[\"test_loss\"].append(loss)\n",
    "\n",
    "        n_pids = len(torch.unique(particle_id))\n",
    "        nhits_per_pid = {}\n",
    "        single_hit_particles = 0\n",
    "        for pid in torch.unique(particle_id):\n",
    "            nhits = torch.sum(particle_id == pid).item()\n",
    "            if nhits == 1:\n",
    "                single_hit_particles += 1\n",
    "            nhits_per_pid[pid.item()] = nhits\n",
    "\n",
    "        db = DBSCAN(eps=eps_per_sector[sector], min_samples=2).fit(xc)\n",
    "        labels = db.labels_\n",
    "        n_clusters = len(np.unique(labels))\n",
    "        cmap = list(cm.tab20c(np.linspace(0, 1, n_clusters)))\n",
    "\n",
    "        perfect_matches = 0\n",
    "        double_majority = 0\n",
    "        lhc_loose_matches = 0\n",
    "        for label in np.unique(labels):\n",
    "            # analyze reconstruction\n",
    "            pid_cluster = particle_id[labels == label].tolist()\n",
    "            N = len(pid_cluster)\n",
    "            pid_counts = Counter(pid_cluster)\n",
    "\n",
    "            # handle the noise case\n",
    "            if label == -1:\n",
    "                continue\n",
    "\n",
    "            # otherwise, clusters should be tracks\n",
    "            most_common = pid_counts.most_common(1)[0]  # e.g. (810, 8)\n",
    "            ntruth = nhits_per_pid[most_common[0]]\n",
    "            if (N == most_common[1]) and (ntruth == most_common[1]):\n",
    "                perfect_matches += 1\n",
    "            if (most_common[1] / N > 0.5) and (most_common[1] / ntruth > 0.5):\n",
    "                double_majority += 1\n",
    "            if most_common[1] / ntruth > 0.75:\n",
    "                lhc_loose_matches += 1\n",
    "\n",
    "        summary_stats[\"n_pids\"] += n_pids - single_hit_particles\n",
    "        summary_stats[\"perfect_matches\"] += perfect_matches\n",
    "        summary_stats[\"double_majority\"] += double_majority\n",
    "        summary_stats[\"lhc_loose_matches\"] += lhc_loose_matches\n",
    "\n",
    "# entire event summary\n",
    "print(\"summary of full data sample\")\n",
    "n_pids = summary_stats[\"n_pids\"]\n",
    "print(\"perfect match fraction:\", summary_stats[\"perfect_matches\"] / n_pids)\n",
    "print(\"double majority fraction:\", summary_stats[\"double_majority\"] / n_pids)\n",
    "print(\"lhc loose fraction:\", summary_stats[\"lhc_loose_matches\"] / n_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795b15c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
