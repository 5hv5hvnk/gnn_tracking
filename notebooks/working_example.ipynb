{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0531c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import collections\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import uniform\n",
    "from torch_geometric.loader import DataLoader\n",
    "import colorlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn_tracking.utils.plotting import EventPlotter\n",
    "\n",
    "n_evts, n_sectors = 20, 32\n",
    "savefig = False\n",
    "indir='/tigress/jdezoort/codalab/train_1'\n",
    "event_plotter = EventPlotter(indir=indir)\n",
    "event_plotter.plot_ep_rv_uv(evtid=21289, savefig=savefig,\n",
    "                            filename='../plots/full_event.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde54fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn_tracking.preprocessing.point_cloud_builder import PointCloudBuilder\n",
    "\n",
    "# build point clouds for each sector in the pixel layers only\n",
    "pc_builder = PointCloudBuilder(indir='/tigress/jdezoort/codalab/train_1',\n",
    "                               outdir='../point_clouds/',\n",
    "                               n_sectors=n_sectors, pixel_only=True, \n",
    "                               redo=False, measurement_mode=False,\n",
    "                               sector_di=0, sector_ds=1.3, thld=0.9,\n",
    "                               log_level=1)\n",
    "pc_builder.process(n=n_evts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2020ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each point cloud is a PyG Data object \n",
    "point_cloud = pc_builder.data_list\n",
    "pc_builder.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn_tracking.utils.plotting import PointCloudPlotter\n",
    "\n",
    "# visualize the sectors in each event and an overlapped ('extended') sector\n",
    "pc_plotter = PointCloudPlotter('../point_clouds', \n",
    "                               n_sectors=pc_builder.n_sectors)\n",
    "pc_plotter.plot_ep_rv_uv_all_sectors(21289, savefig=savefig, filename='../plots/point_cloud.pdf')\n",
    "pc_plotter.plot_ep_rv_uv_with_boundary(21289, 18, \n",
    "                                       pc_builder.sector_di,\n",
    "                                       pc_builder.sector_ds,\n",
    "                                       savefig=savefig, \n",
    "                                       filename='../plots/point_cloud_extended.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91897cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can build graphs on the point clouds using geometric cuts\n",
    "from gnn_tracking.graph_construction.graph_builder import GraphBuilder\n",
    "\n",
    "graph_builder = GraphBuilder(indir='../point_clouds/', outdir='../graphs', \n",
    "                             redo=False, measurement_mode=False, \n",
    "                             phi_slope_max=0.0035, z0_max=200, dR_max=2.3)\n",
    "graph_builder.process(n=n_evts*n_sectors)\n",
    "graph_builder.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeefbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn_tracking.utils.plotting import GraphPlotter\n",
    "\n",
    "# the graph plotter shows the true and false edges constructed by the builder\n",
    "graph_plotter = GraphPlotter(indir='../graphs')\n",
    "graph = graph_builder.data_list[0]\n",
    "print(graph)\n",
    "evtid, s = graph.evtid.item(), graph.s.item()\n",
    "\n",
    "#graph_plotter.plot_rz(graph_builder.data_list[0], \n",
    "#                      f'event{evtid}_s{s}', \n",
    "#                      scale=np.array([1,1,1]))\n",
    "\n",
    "graph_plotter.plot_ep_rz_uv(graph, sector=s, name=f'data{evtid}_s{s}',\n",
    "                            savefig=savefig, filename='../plots/graphs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c987bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn_tracking.models.track_condensation_networks import GraphTCN\n",
    "from gnn_tracking.training.tcn_trainer import TCNTrainer\n",
    "from gnn_tracking.utils.losses import (\n",
    "    EdgeWeightBCELoss,\n",
    "    EdgeWeightFocalLoss,\n",
    "    PotentialLoss,\n",
    "    BackgroundLoss,\n",
    "    ObjectLoss,\n",
    ")\n",
    "\n",
    "# use cuda (gpu) if possible, otherwise fallback to cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f'Utilizing {device}')\n",
    "\n",
    "# use reference graph to get relevant dimensions \n",
    "g = graph_builder.data_list[0]\n",
    "node_indim = g.x.shape[1]\n",
    "edge_indim = g.edge_attr.shape[1]\n",
    "hc_outdim = 2 # output dim of latent space  \n",
    "\n",
    "# partition graphs into train, test, val splits\n",
    "graphs = graph_builder.data_list\n",
    "n_graphs = len(graphs)\n",
    "rand_array = uniform(low=0, high=1, size=n_graphs)\n",
    "train_graphs = [g for i, g in enumerate(graphs) if (rand_array<=0.6)[i]]\n",
    "test_graphs = [g for i, g in enumerate(graphs) if ((rand_array>0.6) & (rand_array<=0.8))[i]]\n",
    "val_graphs = [g for i, g in enumerate(graphs) if (rand_array>0.8)[i]]\n",
    "\n",
    "# build graph loaders\n",
    "params = {'batch_size': 1, 'shuffle': True, 'num_workers': 2}\n",
    "train_loader = DataLoader(list(train_graphs), **params)\n",
    "params = {'batch_size': 1, 'shuffle': False, 'num_workers': 2}\n",
    "test_loader = DataLoader(list(test_graphs), **params)\n",
    "val_loader = DataLoader(list(val_graphs), **params)\n",
    "loaders = {'train': train_loader, 'test': test_loader,\n",
    "           'val': val_loader}\n",
    "print('Loader sizes:', [(k, len(v)) for k, v in loaders.items()])\n",
    "\n",
    "# build loss function dictionary\n",
    "q_min, sb = 0.01, 0.1\n",
    "loss_functions = {\n",
    "    \"edge\": EdgeWeightFocalLoss(gamma=2, alpha=0.25).to(device),\n",
    "    \"potential\": PotentialLoss(q_min=q_min, device=device),\n",
    "    \"background\": BackgroundLoss(device=device, sb=sb),\n",
    "    #\"object\": ObjectLoss(device=device, mode='efficiency')\n",
    "}\n",
    "\n",
    "loss_weights = {\n",
    "    # everything that's not mentioned here will be 1\n",
    "    \"edge\": 5,\n",
    "    \"potential_attractive\": 10,\n",
    "    \"potential_repulsive\": 1,\n",
    "    \"background\": 5,\n",
    "    #\"object\": 1/250000,\n",
    "}\n",
    "\n",
    "# set up a model and trainer\n",
    "model = GraphTCN(node_indim, edge_indim, hc_outdim, hidden_dim=64)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('number trainable params:', n_params)\n",
    "trainer = TCNTrainer(model=model, loaders=loaders, loss_functions=loss_functions,\n",
    "                          lr=0.0001, loss_weights=loss_weights, device=device)\n",
    "print(trainer.loss_functions)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "model.eval()\n",
    "out = model(g.to(device))\n",
    "h = out['H'].detach().cpu().numpy()\n",
    "beta = out['B'].detach().cpu().numpy()\n",
    "w = out['W']\n",
    "particle_id = g.particle_id.cpu()\n",
    "\n",
    "class DBScanner():\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        self.opt_clusters = None\n",
    "        \n",
    "        self.metrics = collections.defaultdict(list)\n",
    "        self.eps = []\n",
    "        self.min_pts = []\n",
    "            \n",
    "    def scan(self, h, particle_id, n_trials=100):\n",
    "        for _ in range(n_trials):\n",
    "            eps = np.random.uniform()\n",
    "            min_pts = np.random.randint(1, 5)\n",
    "            self.eps.append(eps)\n",
    "            self.min_pts.append(min_pts)\n",
    "            db = DBSCAN(eps=eps, min_samples=min_pts).fit(h)\n",
    "            c = db.labels_\n",
    "            e1 = self.get_effs(c, h, particle_id)\n",
    "            e2 = self.get_effs2(c, h, particle_id)\n",
    "            effs = {**e1, **e2}\n",
    "            for k, v in effs.items():\n",
    "                self.metrics[k].append(v)\n",
    "            self.clabels = db.labels_\n",
    "   \n",
    "    def get_effs(self, c, h, particle_id):\n",
    "        c_id = pd.DataFrame({'c': c, 'id': particle_id})\n",
    "        clusters = c_id.groupby('c')\n",
    "        majority_pid = clusters['id'].apply(lambda x: x.mode()[0])\n",
    "        majority_counts = clusters['id'].apply(lambda x: sum(x==x.mode()[0]))\n",
    "        majority_fraction = clusters['id'].apply(lambda x: sum(x==x.mode()[0])/len(x))\n",
    "        h_id = pd.DataFrame({'hits': np.ones(len(h)), 'id': particle_id})\n",
    "        particles = h_id.groupby('id')\n",
    "        nhits = particles['hits'].apply(lambda x: len(x)).to_dict()\n",
    "        majority_hits = clusters['id'].apply(lambda x: x.mode().map(nhits)[0])\n",
    "        perfect_match = ((majority_hits==majority_counts) &\n",
    "                         (majority_fraction > 0.99))\n",
    "        double_majority = (((majority_counts / majority_hits).fillna(0) > 0.5) &\n",
    "                           (majority_fraction > 0.5))\n",
    "        lhc_match = ((majority_fraction).fillna(0) > 0.75)\n",
    "        return {\n",
    "            'total': len(np.unique(c)),\n",
    "            'perfect': sum(perfect_match),\n",
    "            'double_majority': sum(double_majority),\n",
    "            'lhc': sum(lhc_match),\n",
    "        }\n",
    "\n",
    "    def get_effs2(self, c, h, particle_id):\n",
    "        labels, labels_true = c, particle_id\n",
    "        return {\n",
    "            \"homogeneity\": metrics.homogeneity_score(labels_true, labels),\n",
    "            \"completeness\":  metrics.completeness_score(labels_true, labels),\n",
    "            \"v_measure\":  metrics.v_measure_score(labels_true, labels),\n",
    "            \"adjusted_rand_index\":  metrics.adjusted_rand_score(labels_true, labels),\n",
    "        }\n",
    "    \n",
    "x = pd.Series([1, 2, 2, 3])\n",
    "print(x.mode()[0])\n",
    "dbscanner = DBScanner()\n",
    "pt = g.pt.cpu().detach().numpy()\n",
    "dbscanner.scan(h[pt > 1], particle_id[pt > 1], n_trials=10)\n",
    "\n",
    "df = pd.DataFrame(dbscanner.metrics)\n",
    "df[\"dmn\"] = df[\"double_majority\"] / df[\"total\"]\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "sns.pairplot(df, x_vars=[\"dmn\"], \n",
    "             y_vars=[\"homogeneity\", \"completeness\", \"v_measure\", \"adjusted_rand_index\"], aspect=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52626032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.optim.lr_scheduler import StepLR \n",
    "from gnn_tracking.models.track_condensation_networks import PointCloudTCN\n",
    "from gnn_tracking.training.tcn_trainer import TCNTrainer\n",
    "from gnn_tracking.utils.losses import (\n",
    "    EdgeWeightBCELoss,\n",
    "    PotentialLoss,\n",
    "    BackgroundLoss,\n",
    "    ObjectLoss,\n",
    ")\n",
    "\n",
    "# use cuda (gpu) if possible, otherwise fallback to cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f'Utilizing {device}')\n",
    "\n",
    "# use reference graph to get relevant dimensions \n",
    "p = pc_builder.data_list[0]\n",
    "node_indim = p.x.shape[1]\n",
    "hc_outdim = 2 # output dim of latent space  \n",
    "\n",
    "# partition graphs into train, test, val splits\n",
    "point_clouds = pc_builder.data_list\n",
    "n_pcs = len(point_clouds)\n",
    "rand_array = uniform(low=0, high=1, size=n_pcs)\n",
    "train_pcs = [p for i, p in enumerate(point_clouds) if (rand_array<=0.6)[i]]\n",
    "test_pcs = [p for i, p in enumerate(point_clouds) if ((rand_array>0.6) & (rand_array<=0.8))[i]]\n",
    "val_pcs = [p for i, p in enumerate(point_clouds) if (rand_array>0.8)[i]]\n",
    "\n",
    "# build graph loaders\n",
    "params = {'batch_size': 1, 'shuffle': True, 'num_workers': 2}\n",
    "train_loader = DataLoader(list(train_pcs), **params)\n",
    "params = {'batch_size': 1, 'shuffle': False, 'num_workers': 2}\n",
    "test_loader = DataLoader(list(test_pcs), **params)\n",
    "val_loader = DataLoader(list(val_pcs), **params)\n",
    "loaders = {'train': train_loader, 'test': test_loader,\n",
    "           'val': val_loader}\n",
    "print('Loader sizes:', [(k, len(v)) for k, v in loaders.items()])\n",
    "\n",
    "# build loss function dictionary\n",
    "q_min, sb = 0.01, 0.1\n",
    "loss_functions = {\n",
    "    \"potential\": PotentialLoss(q_min=q_min, device=device),\n",
    "    \"background\": BackgroundLoss(device=device, sb=sb),\n",
    "    #\"object\": ObjectLoss(device=device, mode='efficiency')\n",
    "}\n",
    "\n",
    "loss_weights = {\n",
    "    # everything that's not mentioned here will be 1\n",
    "    \"potential_attractive\": 1,\n",
    "    \"potential_repulsive\": 10,\n",
    "    \"background\": 1/10,\n",
    "    #\"object\": 1/2500,\n",
    "}\n",
    "\n",
    "# set up a model and trainer\n",
    "model = PointCloudTCN(node_indim, h_dim=8, e_dim=8, h_outdim=3, L=3, N_blocks=4, hidden_dim=100)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('number trainable params:', n_params)\n",
    "trainer = TCNTrainer(model=model, loaders=loaders, loss_functions=loss_functions,\n",
    "                     lr=0.001, loss_weights=loss_weights, device=device,\n",
    "                     lr_scheduler=partial(StepLR, gamma=0.9, step_size=5))\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705de641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0cfdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0198962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effs(c, h, particle_id):\n",
    "    c_id = pd.DataFrame({'c': c, 'id': particle_id})\n",
    "    clusters = c_id.groupby('c')\n",
    "    majority_pid = clusters['id'].apply(lambda x: x.mode()[0])\n",
    "    majority_counts = clusters['id'].apply(lambda x: sum(x==x.mode()[0]))\n",
    "    majority_fraction = clusters['id'].apply(lambda x: sum(x==x.mode()[0])/len(x))\n",
    "    h_id = pd.DataFrame({'hits': np.ones(len(h)), 'id': particle_id})\n",
    "    particles = h_id.groupby('id')\n",
    "    nhits = particles['hits'].apply(lambda x: len(x)).to_dict()\n",
    "    majority_hits = clusters['id'].apply(lambda x: x.mode().map(nhits)[0])\n",
    "    perfect_match = ((majority_hits==majority_counts) &\n",
    "                     (majority_fraction > 0.99))\n",
    "    double_majority = (((majority_counts / majority_hits).fillna(0) > 0.5) &\n",
    "                       (majority_fraction > 0.5))\n",
    "    lhc_match = ((majority_fraction).fillna(0) > 0.75)\n",
    "    return {\n",
    "        'total': len(np.unique(c)),\n",
    "        'perfect': sum(perfect_match),\n",
    "        'double_majority': sum(double_majority),\n",
    "        'lhc': sum(lhc_match),\n",
    "    }\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "h = np.array([[0.1, 0.2], [0.2, 0.1], [0.3, 0.1], [0.2, 0.2,], [0.3, 0.3],\n",
    "              [0.4, 0.5], [0.5, 0.6], [0.6, 0.6]])\n",
    "plt.plot(h[:,0], h[:,1], '.')\n",
    "particle_id = [0,0,0,0,0,1,1,1]\n",
    "c = [-1,-1, 0, 0, 0, 1 , 1, 2]\n",
    "get_effs(c, h, particle_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
