{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f7918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "from trackml.dataset import load_event\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "import mplhep as hep\n",
    "hep.style.use(\"CMS\")\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad36a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evtid = '21512'\n",
    "prefix = f'/tigress/jdezoort/codalab/train_1/event0000{evtid}'\n",
    "hits, particles, truth = load_event(\n",
    "        prefix, parts=['hits', 'particles', 'truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c01f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, Data\n",
    "\n",
    "def calc_eta(r, z):\n",
    "    theta = np.arctan2(r, z)\n",
    "    eta = -1.0 * np.log(np.tan(theta / 2.0))\n",
    "\n",
    "def append_features(hits, particles, truth):\n",
    "    particles['pt'] = np.sqrt(particles.px**2 + \n",
    "                              particles.py**2)\n",
    "    particles['eta_pt'] = calc_eta(particles.pt, \n",
    "                                   particles.pz)\n",
    "    truth = (truth[['hit_id', 'particle_id']]\n",
    "             .merge(particles[['particle_id', 'pt', 'eta_pt', 'q', 'vx', 'vy']], \n",
    "                    on='particle_id'))\n",
    "    hits['r'] = np.sqrt(hits.x**2 + hits.y**2)\n",
    "    hits['phi'] = np.arctan2(hits.y, hits.x)\n",
    "    hits['eta_rz'] = calc_eta(hits.r, hits.z)\n",
    "    hits['u'] = hits['x']/(hits['x']**2 + hits['y']**2)\n",
    "    hits['v'] = hits['y']/(hits['x']**2 + hits['y']**2)\n",
    "    hits = (hits[['hit_id', 'r', 'phi', 'eta_rz', \n",
    "                  'x', 'y', 'z', 'u', 'v', 'volume_id']]\n",
    "            .merge(truth[['hit_id', 'particle_id', 'pt', 'eta_pt']], \n",
    "                          on='hit_id'))\n",
    "    data = Data(x=hits[['x', 'y', 'z', 'r', 'phi', 'eta_rz', 'u', 'v']].values,\n",
    "                particle_id=hits['particle_id'].values, \n",
    "                pt=hits['pt'].values)\n",
    "    return data\n",
    "\n",
    "class TrackClouds(Dataset):\n",
    "    def __init__(self, root: str, processed_file_dir: str,\n",
    "                 n_sectors: int, pre_transform=None):\n",
    "        self.root = root\n",
    "        self.processed_file_dir = processed_file_dir\n",
    "        self.raw_file_path = root\n",
    "        self.processed_file_path = processed_file_dir\n",
    "        self.n_sectors = n_sectors\n",
    "        self.idx_dict = {}\n",
    "        counter = 0\n",
    "        for i in range(1000):\n",
    "            for j in range(self.n_sectors):\n",
    "                self.idx_dict[counter] = (i, j)\n",
    "                \n",
    "        suffix = '-hits.csv.gz'\n",
    "        self.prefixes, self.exists = [], {}\n",
    "        for p in os.listdir(self.raw_file_path):\n",
    "            if str(p).endswith(suffix):\n",
    "                prefix = str(p).replace(suffix, '')\n",
    "                evtid = int(prefix[-9:])\n",
    "                if f'data{evtid}_s0.pt' in self.processed_file_names:\n",
    "                    self.exists[evtid] = True\n",
    "                else: self.exists[evtid] = False\n",
    "                self.prefixes.append(prefix)\n",
    "        self.dataset = []\n",
    "       \n",
    "        super(TrackClouds, self).__init__(processed_file_dir,\n",
    "                                          pre_transform=pre_transform)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return os.listdir(self.raw_file_path)\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return os.listdir(self.processed_file_path)\n",
    "        \n",
    "    def len(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get(self, idx: int) -> Data:\n",
    "        evtid, s = self.idx_dict[idx]\n",
    "        name = f'data{evtid}_s{s}.pt'\n",
    "        return torch.load(join(self.processed_dir, name))\n",
    "        \n",
    "    def process(self):\n",
    "        idx = 0\n",
    "        for i, f in enumerate(self.prefixes):\n",
    "            s = 0\n",
    "            evtid = int(f[-9:])\n",
    "            name=f'data{evtid}_s{s}.pt'\n",
    "            if self.exists[evtid]: \n",
    "                print(join(self.processed_dir, name))\n",
    "                data = torch.load(join(self.processed_dir, name))\n",
    "                self.dataset.append(data)\n",
    "                continue\n",
    "            print('Processing', evtid)\n",
    "            hits, particles, truth = load_event(\n",
    "                f, parts=['hits', 'particles', 'truth']\n",
    "            )\n",
    "            data = self.pre_transform(hits, particles, truth)\n",
    "            torch.save(data, join(self.processed_dir, name))\n",
    "            self.dataset.append(data)\n",
    "            idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b344b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class PointClouds():\n",
    "    def __init__(self, outdir: str, indir: str,\n",
    "                 n_sectors: int, redo=False):\n",
    "        self.outdir = outdir\n",
    "        self.indir = indir\n",
    "        self.n_sectors = n_sectors\n",
    "        self.redo = redo\n",
    "        self.idx_dict = {}\n",
    "        counter = 0\n",
    "        for i in range(1000):\n",
    "            for j in range(self.n_sectors):\n",
    "                self.idx_dict[counter] = (i, j)\n",
    "                \n",
    "        suffix = '-hits.csv.gz'\n",
    "        self.prefixes, self.exists = [], {}\n",
    "        for p in os.listdir(self.indir):\n",
    "            if str(p).endswith(suffix):\n",
    "                prefix = str(p).replace(suffix, '')\n",
    "                evtid = int(prefix[-9:])\n",
    "                if f'data{evtid}_s0.pt' in os.listdir(outdir):\n",
    "                    self.exists[evtid] = True\n",
    "                else: self.exists[evtid] = False\n",
    "                self.prefixes.append(join(indir, prefix))\n",
    "                \n",
    "        self.data_list = []\n",
    "        self.process()\n",
    "    \n",
    "    def calc_eta(self, r, z):\n",
    "        theta = np.arctan2(r, z)\n",
    "        return -1.0 * np.log(np.tan(theta / 2.0))\n",
    "                \n",
    "    def append_features(self, hits, particles, truth):\n",
    "        particles['pt'] = np.sqrt(particles.px**2 + \n",
    "                                  particles.py**2)\n",
    "        particles['eta_pt'] = self.calc_eta(particles.pt, \n",
    "                                            particles.pz)\n",
    "        truth = (truth[['hit_id', 'particle_id']]\n",
    "                 .merge(particles[['particle_id', 'pt', 'eta_pt', 'q', 'vx', 'vy']], \n",
    "                        on='particle_id'))\n",
    "        hits['r'] = np.sqrt(hits.x**2 + hits.y**2)\n",
    "        hits['phi'] = np.arctan2(hits.y, hits.x)\n",
    "        hits['eta_rz'] = self.calc_eta(hits.r, hits.z)\n",
    "        hits['u'] = hits['x']/(hits['x']**2 + hits['y']**2)\n",
    "        hits['v'] = hits['y']/(hits['x']**2 + hits['y']**2)\n",
    "        hits = (hits[['hit_id', 'r', 'phi', 'eta_rz', \n",
    "                      'x', 'y', 'z', 'u', 'v', 'volume_id']]\n",
    "                .merge(truth[['hit_id', 'particle_id', 'pt', 'eta_pt']], \n",
    "                              on='hit_id'))\n",
    "        data = Data(x=hits[['x', 'y', 'z', 'r', 'phi', 'eta_rz', 'u', 'v']].values,\n",
    "                    particle_id=hits['particle_id'].values, \n",
    "                    pt=hits['pt'].values)\n",
    "        return data\n",
    "\n",
    "    def process(self):\n",
    "        for i, f in enumerate(self.prefixes):\n",
    "            print(f)\n",
    "            s = 0\n",
    "            evtid = int(f[-9:])\n",
    "            name=f'data{evtid}_s{s}.pt'\n",
    "            if self.exists[evtid] and not self.redo:\n",
    "                data = torch.load(join(self.outdir, name))\n",
    "                self.data_list.append(data)\n",
    "            else: \n",
    "                hits, particles, truth = load_event(\n",
    "                    f, parts=['hits', 'particles', 'truth']\n",
    "                )\n",
    "                data = self.append_features(hits, particles, truth)\n",
    "                torch.save(data, join(self.outdir, name))\n",
    "                self.data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b11b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataListLoader\n",
    "tc = PointClouds(indir='/tigress/jdezoort/codalab/train_1', outdir='../point_clouds/',\n",
    "                 n_sectors=1)\n",
    "for i in tc:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edec089",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataListLoader(tc.data_list, batch_size=1)\n",
    "for l in loader: print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cac14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "\n",
    "class TrackClouds(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return os.listdir(self.root)\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data{e}_s0.pt' \n",
    "                for e in np.arange(21000, 22000, 1)]\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c6e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "        #prefixes = [str(p).replace(suffix, '')\n",
    "        #            for p in self.raw_file_path.iterdir()\n",
    "        #            if str(p).endswith(suffix)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
